{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yunjue Agent Benchmark - A100 GPU Acceleration\n",
    "\n",
    "This notebook runs the financial agent benchmark using local LLM inference on A100 80G.\n",
    "\n",
    "**Requirements**: Colab Pro+ with A100 GPU runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/YOUR_REPO/insitu-finance-agent.git\n",
    "%cd insitu-finance-agent/fin_evo_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install base dependencies\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install GPU dependencies for local LLM\n",
    "!pip install -q -r requirements-gpu.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Option A: vLLM Server (Recommended)\n",
    "\n",
    "vLLM provides OpenAI-compatible API with optimal GPU utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install vLLM\n",
    "!pip install -q vllm>=0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Choose model - Qwen2.5-Coder is excellent for code generation\n",
    "# For A100 80G, we can run 32B+ models\n",
    "MODEL_ID = \"Qwen/Qwen2.5-Coder-32B-Instruct\"  # Best for code\n",
    "# Alternative: \"deepseek-ai/deepseek-coder-33b-instruct\"\n",
    "# Alternative: \"Qwen/Qwen2.5-72B-Instruct\"  # For more reasoning\n",
    "\n",
    "# Start vLLM server in background\n",
    "vllm_process = subprocess.Popen(\n",
    "    [\n",
    "        \"python\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n",
    "        \"--model\", MODEL_ID,\n",
    "        \"--host\", \"0.0.0.0\",\n",
    "        \"--port\", \"8000\",\n",
    "        \"--tensor-parallel-size\", \"1\",  # Single A100\n",
    "        \"--gpu-memory-utilization\", \"0.90\",\n",
    "        \"--max-model-len\", \"8192\",\n",
    "        \"--trust-remote-code\",\n",
    "    ],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE\n",
    ")\n",
    "\n",
    "print(\"Starting vLLM server...\")\n",
    "time.sleep(60)  # Wait for model to load\n",
    "print(\"vLLM server should be ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test vLLM endpoint\n",
    "!curl -s http://localhost:8000/v1/models | python -m json.tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure environment for local LLM\n",
    "import os\n",
    "\n",
    "os.environ[\"LLM_TYPE\"] = \"local\"\n",
    "os.environ[\"LLM_BASE_URL\"] = \"http://localhost:8000/v1\"\n",
    "os.environ[\"LLM_MODEL\"] = MODEL_ID\n",
    "os.environ[\"API_KEY\"] = \"not-needed\"  # vLLM doesn't require API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Option B: Direct Transformers (Alternative)\n",
    "\n",
    "If vLLM doesn't work, use direct transformers integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip this cell if using vLLM\n",
    "# This is for direct transformers usage\n",
    "\n",
    "import os\n",
    "os.environ[\"LLM_TYPE\"] = \"transformers\"\n",
    "os.environ[\"LLM_MODEL_ID\"] = \"Qwen/Qwen2.5-Coder-32B-Instruct\"\n",
    "os.environ[\"LLM_QUANTIZATION\"] = \"none\"  # A100 80G can handle full precision\n",
    "# Use \"4bit\" or \"8bit\" if you want faster inference with slight quality tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "!python benchmarks/run_eval.py --config cold_start --run-id colab_a100_run\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Total benchmark time: {elapsed:.1f}s ({elapsed/60:.1f} minutes)\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View results\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "results_file = Path(\"benchmarks/results/colab_a100_run.json\")\n",
    "if results_file.exists():\n",
    "    with open(results_file) as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    print(f\"Pass Rate: {results['summary']['pass_rate']*100:.1f}%\")\n",
    "    print(f\"Passed: {results['summary']['passed']}/{results['summary']['total']}\")\n",
    "    print(f\"\\nBy Category:\")\n",
    "    for cat, stats in results['summary'].get('by_category', {}).items():\n",
    "        print(f\"  {cat}: {stats['passed']}/{stats['total']}\")\n",
    "else:\n",
    "    print(\"Results file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. GPU Memory Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU memory during/after benchmark\n",
    "!nvidia-smi --query-gpu=name,memory.used,memory.total,utilization.gpu --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop vLLM server if running\n",
    "try:\n",
    "    vllm_process.terminate()\n",
    "    print(\"vLLM server stopped\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "\n",
    "| Setup | Model | Time (20 tasks) | Pass Rate |\n",
    "|-------|-------|-----------------|----------|\n",
    "| API (Qwen3-Max) | qwen3-max | ~180s | 85% |\n",
    "| A100 + vLLM | Qwen2.5-Coder-32B | ~60s | TBD |\n",
    "| A100 + Transformers | Qwen2.5-Coder-32B | ~90s | TBD |"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
