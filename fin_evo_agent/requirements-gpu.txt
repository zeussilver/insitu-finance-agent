# GPU dependencies for local LLM inference
# Install with: pip install -r requirements-gpu.txt

# PyTorch with CUDA support (use appropriate version for your CUDA)
torch>=2.1.0
# For specific CUDA versions, install manually:
# pip install torch --index-url https://download.pytorch.org/whl/cu121

# HuggingFace ecosystem
transformers>=4.40.0
accelerate>=0.27.0
safetensors>=0.4.0

# Quantization support (optional, for memory optimization)
bitsandbytes>=0.42.0  # For 4-bit/8-bit quantization

# vLLM for high-throughput inference (recommended)
vllm>=0.4.0

# Flash Attention (optional, for faster inference)
# Install separately: pip install flash-attn --no-build-isolation

# Monitoring
gpustat>=1.1.0

# Tokenizers
sentencepiece>=0.1.99
tiktoken>=0.5.0
