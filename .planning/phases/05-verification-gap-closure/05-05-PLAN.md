---
phase: 05-verification-gap-closure
plan: 05
type: execute
wave: 3
depends_on:
  - "05-03"
  - "05-04"
files_modified:
  - fin_evo_agent/benchmarks/run_eval.py
autonomous: true

must_haves:
  truths:
    - "run_eval.py uses TaskExecutor for task execution"
    - "run_eval.py uses schema-based matching instead of keyword matching"
    - "Fetch tasks work via bootstrap tool chaining"
    - "All existing evaluation features still work (JSON output, colors, baseline)"
  artifacts:
    - path: "fin_evo_agent/benchmarks/run_eval.py"
      provides: "Updated evaluation with TaskExecutor and schema matching"
      contains: "TaskExecutor"
  key_links:
    - from: "run_eval.py EvalRunner"
      to: "TaskExecutor.execute_task()"
      via: "task_executor instance"
      pattern: "task_executor\\.execute_task"
    - from: "run_eval.py tool finding"
      to: "registry.find_by_schema()"
      via: "schema-based matching"
      pattern: "find_by_schema"
---

<objective>
Update run_eval.py to use TaskExecutor for task execution and schema-based tool matching.

Purpose: Replace the current keyword-based `_infer_tool_name()` with schema-based matching via `registry.find_by_schema()`. Use TaskExecutor to handle fetch/calc chaining so generated tools stay pure while fetch tasks work correctly.

Output: Updated benchmarks/run_eval.py that integrates TaskExecutor and structured matching.
</objective>

<execution_context>
@/Users/liuzhenqian/.claude/get-shit-done/workflows/execute-plan.md
@/Users/liuzhenqian/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-verification-gap-closure/05-CONTEXT.md
@.planning/phases/05-verification-gap-closure/05-RESEARCH.md
@fin_evo_agent/benchmarks/run_eval.py
@fin_evo_agent/src/core/task_executor.py
@fin_evo_agent/src/core/registry.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add TaskExecutor and schema matching to EvalRunner</name>
  <files>fin_evo_agent/benchmarks/run_eval.py</files>
  <action>
Update `run_eval.py` with the following changes:

1. **Add import** at the top (after existing imports):
```python
from src.core.task_executor import TaskExecutor
```

2. **Update EvalRunner.__init__()** to create a TaskExecutor instance:
Add after `self.synthesizer = Synthesizer(...)`:
```python
self.task_executor = TaskExecutor(self.registry, self.executor)
```

3. **Add schema extraction helper method** to EvalRunner class:
```python
def _extract_schema_from_task(self, task: dict) -> dict:
    """Extract schema fields from task for tool matching."""
    query = task.get('query', '').lower()
    category = task.get('category', 'calculation')

    # Extract indicator type
    indicator = None
    if 'rsi' in query:
        indicator = 'rsi'
    elif 'macd' in query:
        indicator = 'macd'
    elif 'bollinger' in query or '布林' in query:
        indicator = 'bollinger'
    elif 'kdj' in query:
        indicator = 'kdj'
    elif '波动率' in query or 'volatility' in query:
        indicator = 'volatility'
    elif '回撤' in query or 'drawdown' in query:
        indicator = 'drawdown'
    elif '相关' in query or 'correlation' in query:
        indicator = 'correlation'
    elif '量价' in query or 'divergence' in query:
        indicator = 'volume_price'
    elif '组合' in query or 'portfolio' in query:
        indicator = 'portfolio'
    elif 'ma' in query and 'macd' not in query:
        indicator = 'ma'

    return {
        'category': category,
        'indicator': indicator,
    }
```

4. **Update `run_task()` method** - replace the tool finding logic:

Find this section (around line 558-566):
```python
# 2. Try to retrieve existing tool
tool_name = self._infer_tool_name(query)
tool = None

if tool_name:
    tool = self.registry.get_by_name(tool_name)
```

Replace with:
```python
# 2. Try to retrieve existing tool using schema-based matching
schema = self._extract_schema_from_task(task)
tool = None

# First try schema-based matching
if schema['indicator']:
    tool = self.registry.find_by_schema(
        category=schema['category'],
        indicator=schema['indicator']
    )
    if tool:
        print(f"  > Found tool via schema: {tool.name} (indicator={schema['indicator']})")
        result["tool_source"] = "reused"
        result["generated_code"] = tool.code_content

# Fallback to keyword-based matching if schema match fails
if not tool:
    tool_name = self._infer_tool_name(query)
    if tool_name:
        tool = self.registry.get_by_name(tool_name)
        if tool:
            print(f"  > Found tool via name: {tool.name} v{tool.semantic_version}")
            result["tool_source"] = "reused"
            result["generated_code"] = tool.code_content
```

5. **Update the tool execution section** - use TaskExecutor instead of direct executor:

Find this section (around line 598-634) where tool execution happens:
```python
# 4. Execute the tool
# Extract stock symbol from query if present
...
trace = self.executor.execute(...)
```

Replace with:
```python
# 4. Execute the tool using TaskExecutor
trace = self.task_executor.execute_task(task, tool)
```

This replaces all the manual data fetching and argument preparation with TaskExecutor's standardized flow.

6. **Keep the result extraction and classification** unchanged - those sections after execution should remain as they are.
  </action>
  <verify>
Run:
```bash
cd fin_evo_agent && python -c "
import sys
sys.path.insert(0, '.')

# Check imports work
from benchmarks.run_eval import EvalRunner

# Check TaskExecutor is used
import inspect
init_source = inspect.getsource(EvalRunner.__init__)
assert 'task_executor' in init_source.lower() or 'TaskExecutor' in init_source, 'Missing TaskExecutor in __init__'

# Check schema extraction method exists
assert hasattr(EvalRunner, '_extract_schema_from_task'), 'Missing _extract_schema_from_task method'

# Check run_task uses schema matching
run_task_source = inspect.getsource(EvalRunner.run_task)
assert 'find_by_schema' in run_task_source, 'Missing find_by_schema in run_task'
assert 'task_executor' in run_task_source.lower() or 'execute_task' in run_task_source, 'Missing task_executor.execute_task in run_task'

print('EvalRunner integration: PASS')
"
```
  </verify>
  <done>
- EvalRunner has self.task_executor instance
- _extract_schema_from_task() extracts category and indicator from task
- run_task() tries schema-based matching first, falls back to keyword matching
- Tool execution uses task_executor.execute_task() instead of direct executor
  </done>
</task>

<task type="auto">
  <name>Task 2: Run quick smoke test</name>
  <files>fin_evo_agent/benchmarks/run_eval.py</files>
  <action>
Run a quick smoke test to verify the changes work:

1. Initialize database and run a single task to verify flow works
2. Check that existing features (JSON output, colors) still work

```bash
# Quick test with first 3 tasks only
cd fin_evo_agent && python -c "
import sys
sys.path.insert(0, '.')
from benchmarks.run_eval import EvalRunner

print('Creating EvalRunner...')
runner = EvalRunner('evolving', 'smoke_test')

# Run just one calc task to verify flow
task = {
    'task_id': 'smoke_001',
    'category': 'calculation',
    'query': '计算AAPL的RSI-14指标',
    'expected_output': {'type': 'numeric'}
}

print('Running task...')
result = runner.run_task(task, 1, 1)
print(f'Result: state={result[\"state\"]}, tool_source={result[\"tool_source\"]}')

if result['state'] == 'pass' or result['tool_source'] != 'failed':
    print('Smoke test: PASS')
else:
    print(f'Smoke test: PARTIAL (expected for first run without cached tools)')
"
```
  </action>
  <verify>
The smoke test should show:
- EvalRunner initializes without error
- Task execution completes (may fail on first run without cached tools, but should not crash)
- Output shows proper state classification
  </verify>
  <done>
- EvalRunner initializes correctly with TaskExecutor
- Single task execution completes without crashes
- Schema-based matching attempted (logs show "Found tool via schema" or fallback)
- Error handling works properly
  </done>
</task>

</tasks>

<verification>
1. EvalRunner uses TaskExecutor for execution
2. Schema-based matching is attempted before keyword matching
3. Fetch/calculation/composite tasks all work via TaskExecutor
4. Existing features preserved (JSON output, colors, baseline, regression detection)
5. No crashes on empty registry or new tasks
</verification>

<success_criteria>
- run_eval.py imports and uses TaskExecutor
- _extract_schema_from_task() parses 10+ indicator types
- run_task() tries find_by_schema() before _infer_tool_name()
- Task execution uses task_executor.execute_task()
- Smoke test passes without crashes
</success_criteria>

<output>
After completion, create `.planning/phases/05-verification-gap-closure/05-05-SUMMARY.md`
</output>
