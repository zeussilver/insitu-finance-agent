---
phase: 05-verification-gap-closure
plan: 06
type: execute
wave: 4
depends_on:
  - "05-01"
  - "05-05"
files_modified:
  - .github/workflows/benchmark.yml
autonomous: true

must_haves:
  truths:
    - "GitHub Actions workflow runs on PR to main and manual dispatch"
    - "Workflow runs benchmark evaluation with API key from secrets"
    - "Workflow fails on regressions (baseline tasks now failing)"
    - "Workflow posts results as PR comment"
    - "Benchmark results saved as artifacts"
  artifacts:
    - path: ".github/workflows/benchmark.yml"
      provides: "CI pipeline for benchmark regression testing"
      contains: "run_eval.py"
  key_links:
    - from: "benchmark.yml"
      to: "benchmarks/run_eval.py"
      via: "python command"
      pattern: "python benchmarks/run_eval\\.py"
---

<objective>
Create GitHub Actions CI workflow for automated benchmark regression testing.

Purpose: Catch regressions automatically when code changes are made. The workflow runs on PRs to main and can be triggered manually. It fails hard on regressions (baseline tasks that were passing now fail) but only warns on pass rate < 80% (to account for LLM variance).

Output: New `.github/workflows/benchmark.yml` workflow file with proper caching, secrets handling, and PR comments.
</objective>

<execution_context>
@/Users/liuzhenqian/.claude/get-shit-done/workflows/execute-plan.md
@/Users/liuzhenqian/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-verification-gap-closure/05-CONTEXT.md
@.planning/phases/05-verification-gap-closure/05-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create GitHub Actions workflow</name>
  <files>.github/workflows/benchmark.yml</files>
  <action>
Create the directory structure and workflow file:

```bash
mkdir -p .github/workflows
```

Create `.github/workflows/benchmark.yml` with the following content:

```yaml
# Benchmark Evaluation CI
# Runs on PRs to main and manual dispatch
# Fails on regressions, warns on low pass rate

name: Benchmark Evaluation

on:
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      clear_registry:
        description: 'Clear tool registry before run'
        required: false
        default: 'false'
        type: boolean

jobs:
  benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: fin_evo_agent/requirements.txt

      - name: Install dependencies
        working-directory: fin_evo_agent
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Restore yfinance cache
        uses: actions/cache@v4
        with:
          path: fin_evo_agent/data/cache
          key: yfinance-cache-${{ hashFiles('fin_evo_agent/benchmarks/tasks.jsonl') }}
          restore-keys: |
            yfinance-cache-

      - name: Initialize database
        working-directory: fin_evo_agent
        run: python main.py --init

      - name: Run benchmark evaluation
        working-directory: fin_evo_agent
        env:
          API_KEY: ${{ secrets.API_KEY }}
        run: |
          # Build command based on inputs
          CMD="python benchmarks/run_eval.py --agent evolving --run-id ci-${{ github.run_number }}"

          if [[ "${{ github.event.inputs.clear_registry }}" == "true" ]]; then
            CMD="$CMD --clear-registry"
          fi

          echo "Running: $CMD"
          $CMD

      - name: Check for regressions
        id: check_results
        working-directory: fin_evo_agent
        run: |
          python -c "
          import json
          import sys
          import os

          results_file = 'benchmarks/results/ci-${{ github.run_number }}.json'

          try:
              with open(results_file) as f:
                  results = json.load(f)
          except FileNotFoundError:
              print('ERROR: Results file not found')
              sys.exit(1)

          summary = results.get('summary', {})
          regressions = summary.get('regressions', [])
          pass_rate = summary.get('pass_rate', 0)
          passed = summary.get('passed', 0)
          total = summary.get('total_tasks', 0)

          # Write outputs for PR comment
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'pass_rate={pass_rate:.2%}\n')
              f.write(f'passed={passed}\n')
              f.write(f'total={total}\n')
              f.write(f'regression_count={len(regressions)}\n')

          # Check for regressions (hard fail)
          if regressions:
              print(f'FAIL: {len(regressions)} regressions detected!')
              for r in regressions:
                  print(f\"  - {r['task_id']}: {r.get('failure_reason', 'unknown')[:50]}\")
              sys.exit(1)

          # Check pass rate (warning only)
          if pass_rate < 0.80:
              print(f'WARNING: Pass rate {pass_rate:.1%} below 80% target')
              print('(Not failing - LLM variance expected)')
          else:
              print(f'OK: {pass_rate:.1%} pass rate, no regressions')
          "

      - name: Upload results artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results-${{ github.run_number }}
          path: |
            fin_evo_agent/benchmarks/results/ci-${{ github.run_number }}.json
            fin_evo_agent/data/logs/security_violations.log
          retention-days: 30

      - name: Comment on PR
        if: github.event_name == 'pull_request' && always()
        uses: thollander/actions-comment-pull-request@v2
        with:
          comment_tag: benchmark-results
          message: |
            ## Benchmark Results

            | Metric | Value |
            |--------|-------|
            | Pass Rate | ${{ steps.check_results.outputs.pass_rate }} (${{ steps.check_results.outputs.passed }}/${{ steps.check_results.outputs.total }}) |
            | Regressions | ${{ steps.check_results.outputs.regression_count }} |
            | Target | >= 80% |

            ${{ steps.check_results.outputs.regression_count != '0' && '**FAILED:** Regressions detected! Baseline tasks that were passing are now failing.' || (steps.check_results.outputs.pass_rate < '0.80' && '**WARNING:** Pass rate below target (LLM variance expected)' || '**PASSED:** All checks OK') }}

            [Full results artifact](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})

            ---
            *Generated by Claude Code CI*
```
  </action>
  <verify>
Run:
```bash
# Verify file exists and has correct structure
cat "/Users/liuzhenqian/Desktop/personal project/2026-1-week3/Insitu finance agent/.github/workflows/benchmark.yml" | head -50

# Check YAML validity (basic check)
cd "/Users/liuzhenqian/Desktop/personal project/2026-1-week3/Insitu finance agent" && python -c "
import yaml
with open('.github/workflows/benchmark.yml') as f:
    config = yaml.safe_load(f)

assert 'on' in config, 'Missing on trigger'
assert 'pull_request' in config['on'], 'Missing pull_request trigger'
assert 'workflow_dispatch' in config['on'], 'Missing workflow_dispatch trigger'
assert 'jobs' in config, 'Missing jobs'
assert 'benchmark' in config['jobs'], 'Missing benchmark job'

steps = config['jobs']['benchmark']['steps']
step_names = [s.get('name', '') for s in steps]
assert any('benchmark' in n.lower() for n in step_names), 'Missing benchmark step'
assert any('regression' in n.lower() for n in step_names), 'Missing regression check step'

print('YAML structure: PASS')
print(f'Triggers: {list(config[\"on\"].keys())}')
print(f'Steps: {len(steps)} steps defined')
"
```
  </verify>
  <done>
- .github/workflows/benchmark.yml exists
- Triggers on pull_request to main and workflow_dispatch
- Installs dependencies and runs benchmark
- Uses API_KEY from GitHub Secrets
- Caches yfinance data for reproducibility
- Checks for regressions (hard fail) and pass rate (warning)
- Uploads results artifact
- Posts PR comment with summary
  </done>
</task>

<task type="auto">
  <name>Task 2: Document CI setup for user</name>
  <files>.github/workflows/benchmark.yml</files>
  <action>
Add a comment block at the top of the workflow file explaining setup requirements:

The workflow file should already have this, but verify the header contains:
- How to set up API_KEY secret
- How to trigger manually
- What the workflow does

The header comment should be:
```yaml
# Benchmark Evaluation CI
# ======================
#
# Setup:
#   1. Go to repository Settings -> Secrets and variables -> Actions
#   2. Add repository secret: API_KEY (your DashScope API key)
#
# Triggers:
#   - Automatically on PRs to main branch
#   - Manually via Actions tab -> "Run workflow"
#
# Behavior:
#   - FAILS on regressions (baseline tasks now failing)
#   - WARNS on pass rate < 80% (LLM variance expected)
#   - Posts results as PR comment
#   - Saves results as downloadable artifact
#
# Cache:
#   - yfinance data cached for reproducibility
#   - pip dependencies cached for speed
```

If the header is missing or incomplete, add it at the top of the file.
  </action>
  <verify>
Run:
```bash
head -25 "/Users/liuzhenqian/Desktop/personal project/2026-1-week3/Insitu finance agent/.github/workflows/benchmark.yml"
```

Should show the setup documentation comment block.
  </verify>
  <done>
- Workflow file has clear setup documentation
- Instructions for adding API_KEY secret
- Explanation of triggers and behavior
- Users can set up CI without additional documentation
  </done>
</task>

</tasks>

<verification>
1. `.github/workflows/benchmark.yml` exists with valid YAML
2. Workflow triggers on PR to main and workflow_dispatch
3. Workflow uses API_KEY from secrets
4. Workflow caches yfinance data
5. Workflow fails on regressions, warns on low pass rate
6. Workflow posts PR comment and uploads artifacts
7. Setup documentation is clear
</verification>

<success_criteria>
- GitHub Actions workflow file created at `.github/workflows/benchmark.yml`
- Triggers: pull_request (main), workflow_dispatch
- Environment: Python 3.11, requirements.txt dependencies
- Secrets: API_KEY for LLM API
- Cache: yfinance data in data/cache/
- Checks: Hard fail on regressions, warning on pass rate < 80%
- Outputs: PR comment with results table, artifact with JSON results
</success_criteria>

<output>
After completion, create `.planning/phases/05-verification-gap-closure/05-06-SUMMARY.md`
</output>
