---
phase: 04-regression-verification
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified: []
autonomous: false

must_haves:
  truths:
    - "Full 20-task benchmark completes without crash"
    - "Task success rate >= 80% (16/20 or better)"
    - "All 13 baseline tasks still pass (0 regressions)"
    - "Security block rate = 100% (5/5)"
    - "Results saved to JSON file for future comparison"
  artifacts:
    - path: "fin_evo_agent/benchmarks/results/{run_id}.json"
      provides: "Detailed verification results"
      contains: "summary"
  key_links:
    - from: "run_eval.py"
      to: "evolution pipeline"
      via: "synthesizer.synthesize_with_refine()"
      pattern: "synthesize_with_refine"
---

<objective>
Run the full benchmark verification to confirm all Phase 1-3 fixes achieve the 80% target without regressions.

Purpose: Validate that the complete fix set works together to meet the project's core value (80% task success rate).
Output: Verification results with pass/fail determination for the sprint.
</objective>

<execution_context>
@/Users/liuzhenqian/.claude/get-shit-done/workflows/execute-plan.md
@/Users/liuzhenqian/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-regression-verification/04-CONTEXT.md
@.planning/phases/04-regression-verification/04-01-SUMMARY.md

# Source files
@fin_evo_agent/benchmarks/run_eval.py
@fin_evo_agent/benchmarks/baseline.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Clear registry and run full benchmark verification</name>
  <files></files>
  <action>
Execute the verification run with these steps:

**1. Verify API key is set:**
```bash
if [ -z "$API_KEY" ]; then
  echo "ERROR: API_KEY environment variable not set"
  echo "Set it with: export API_KEY=your_dashscope_api_key"
  exit 1
fi
```

**2. Clear registry for fresh generation test:**
```bash
cd fin_evo_agent
python3 -c "
from benchmarks.run_eval import EvalRunner
runner = EvalRunner('evolving')
runner.clear_registry()
"
```

**3. Run full 20-task benchmark:**
```bash
cd fin_evo_agent
python benchmarks/run_eval.py --agent evolving --run-id verification_phase4 --clear-registry
```

**4. Run security evaluation separately:**
```bash
cd fin_evo_agent
python benchmarks/run_eval.py --security-only
```

**Expected output:**
- Task execution progress with colored pass/fail/error
- Per-category breakdown (fetch/calc/composite)
- Regression detection against baseline
- Summary with pass rate and verdict
- JSON results saved to benchmarks/results/verification_phase4.json

**Important notes:**
- This run will take several minutes (120s timeout per task max)
- If LLM API rate limited, tasks may show ERROR state (not FAIL)
- All 20 tasks run regardless of failures (no early stop)
- Ctrl+C will save partial results
  </action>
  <verify>
```bash
cd fin_evo_agent

# Verify JSON results file was created
ls -la benchmarks/results/verification_phase4.json

# Verify results contain expected structure
python3 -c "
import json
with open('benchmarks/results/verification_phase4.json') as f:
    results = json.load(f)

# Check structure
assert 'summary' in results, 'Missing summary'
assert 'tasks' in results, 'Missing tasks'
assert 'by_category' in results, 'Missing by_category'

# Print summary
print(f\"Pass rate: {results['summary']['pass_rate']*100:.1f}%\")
print(f\"Passed: {results['summary']['passed']}/{results['summary']['total_tasks']}\")
print(f\"Regressions: {len(results['summary'].get('regressions', []))}\")
print(f\"Target met: {results['summary']['target_met']}\")
"
```
  </verify>
  <done>Benchmark run completes and JSON results saved</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: Verify benchmark results meet targets</name>
  <what-built>Full benchmark verification run with Phase 1-3 fixes</what-built>
  <how-to-verify>
Review the verification results:

**1. Check pass rate target (>= 80%):**
- Look at the summary output from the benchmark run
- Pass rate must be >= 16/20 (80%)

**2. Check for regressions (must be 0):**
- Review the regressions section in the summary
- All 13 baseline tasks (8 fetch + 2 calc + 3 composite) must still pass
- Baseline tasks: fetch_001-008, calc_002, calc_007, comp_001, comp_003, comp_004

**3. Check security block rate (must be 100%):**
- Review the security evaluation output
- All 5 security tasks must be blocked

**4. Review any failures:**
- Check which tasks failed and why
- Categorize failures: logic errors vs API/timeout errors

**Success criteria:**
- Pass rate >= 80% (16/20)
- Regressions = 0
- Security block rate = 100%

**If targets NOT met:**
- Report which targets failed
- Analyze failure patterns
- User decides next steps (additional fixes vs accept current state)
  </how-to-verify>
  <resume-signal>Type "approved" if all targets met, or describe which targets failed and what action to take</resume-signal>
</task>

</tasks>

<verification>
Verification is complete when:
1. Benchmark run finishes without crash
2. JSON results saved to benchmarks/results/
3. Human confirms targets met (or provides direction if not)
</verification>

<success_criteria>
- Task success rate >= 80% (REGR-01 requirement)
- 0 regressions on 13 baseline tasks (REGR-01 requirement)
- 100% security block rate (REGR-02 requirement)
- At least 3 of 7 previously-failing tasks now pass
</success_criteria>

<output>
After completion, create `.planning/phases/04-regression-verification/04-02-SUMMARY.md`
</output>
