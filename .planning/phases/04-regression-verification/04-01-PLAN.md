---
phase: 04-regression-verification
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - fin_evo_agent/benchmarks/baseline.json
  - fin_evo_agent/benchmarks/run_eval.py
autonomous: true

must_haves:
  truths:
    - "Baseline.json exists with 13 previously-passing task IDs"
    - "Running eval saves detailed JSON results to benchmarks/results/"
    - "Console output shows colored pass/fail status"
    - "Registry can be cleared before fresh generation test"
    - "Ctrl+C saves partial results before exiting"
    - "Results distinguish pass, fail, and error states"
    - "Regressions detected by comparing against baseline"
  artifacts:
    - path: "fin_evo_agent/benchmarks/baseline.json"
      provides: "13 previously-passing task IDs for regression detection"
      contains: "passing_tasks"
    - path: "fin_evo_agent/benchmarks/run_eval.py"
      provides: "Enhanced evaluation runner with JSON output, colors, baseline comparison"
      exports: ["EvalRunner", "ResultState"]
  key_links:
    - from: "run_eval.py"
      to: "baseline.json"
      via: "json.load() at start of run"
      pattern: "baseline\\.json"
    - from: "run_eval.py"
      to: "benchmarks/results/*.json"
      via: "json.dump() after run completes"
      pattern: "results.*\\.json"
---

<objective>
Enhance the evaluation runner with JSON output, colored console display, baseline comparison for regression detection, registry clearing, and graceful interrupt handling.

Purpose: Enable the verification run to produce detailed, analyzable results with clear pass/fail visibility and regression tracking.
Output: Enhanced run_eval.py and baseline.json file ready for verification runs.
</objective>

<execution_context>
@/Users/liuzhenqian/.claude/get-shit-done/workflows/execute-plan.md
@/Users/liuzhenqian/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-regression-verification/04-CONTEXT.md
@.planning/phases/04-regression-verification/04-RESEARCH.md

# Source files
@fin_evo_agent/benchmarks/run_eval.py
@fin_evo_agent/benchmarks/eval_report_run1_v2.csv
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create baseline.json with 13 passing tasks</name>
  <files>fin_evo_agent/benchmarks/baseline.json</files>
  <action>
Create baseline.json file containing the 13 tasks that passed in eval_report_run1_v2.csv:

```json
{
    "version": "2026-02-02",
    "description": "Baseline from run1_v2.csv - 13 passing tasks before Phase 3 fixes",
    "passing_tasks": [
        "fetch_001", "fetch_002", "fetch_003", "fetch_004",
        "fetch_005", "fetch_006", "fetch_007", "fetch_008",
        "calc_002", "calc_007",
        "comp_001", "comp_003", "comp_004"
    ],
    "total_tasks": 20,
    "target_pass_rate": 0.80
}
```

This file is read-only after creation. It defines which tasks must NOT regress.
  </action>
  <verify>
```bash
cd fin_evo_agent
python3 -c "
import json
with open('benchmarks/baseline.json') as f:
    baseline = json.load(f)
assert len(baseline['passing_tasks']) == 13, f'Expected 13, got {len(baseline[\"passing_tasks\"])}'
assert baseline['target_pass_rate'] == 0.80
print('PASS: baseline.json validated')
"
```
  </verify>
  <done>baseline.json exists with 13 task IDs and 0.80 target rate</done>
</task>

<task type="auto">
  <name>Task 2: Enhance run_eval.py with JSON output, colors, baseline comparison, and interrupt handling</name>
  <files>fin_evo_agent/benchmarks/run_eval.py</files>
  <action>
Add these features to run_eval.py:

**1. ResultState class (three-state classification):**
```python
class ResultState:
    PASS = "pass"
    FAIL = "fail"      # Logic failure (AssertionError, wrong output)
    ERROR = "error"    # External error (API, timeout, network)
```

**2. Colors class (ANSI escape codes):**
```python
class Colors:
    GREEN = "\033[92m"
    RED = "\033[91m"
    YELLOW = "\033[93m"
    CYAN = "\033[96m"
    BOLD = "\033[1m"
    RESET = "\033[0m"
```

**3. Modify EvalRunner.__init__():**
- Add `self.baseline = self._load_baseline()`
- Add `self.interrupted = False`
- Add `self.security_results = {"total": 0, "blocked": 0}`

**4. Add _load_baseline() method:**
- Load benchmarks/baseline.json
- Return dict with passing_tasks list
- Handle missing file gracefully (return empty baseline)

**5. Add clear_registry() method:**
- Delete all ToolArtifact records from database using SQLModel Session
- Delete all .py files from data/artifacts/generated/
- Print "[Setup] Tool registry cleared for fresh generation test"

**6. Add _classify_result() method:**
- Check if exit_code == 0 and output passes judge_result -> PASS
- Check stderr for API/timeout/network indicators -> ERROR
- Otherwise -> FAIL

**7. Add detect_regressions() method:**
- Compare current results against baseline.passing_tasks
- Return list of {task_id, baseline_state: "pass", current_state, failure_reason}

**8. Add save_results_json() method:**
- Create benchmarks/results/ directory if not exists
- Save full JSON with schema from RESEARCH.md:
  - run_id, timestamp, agent_type, config
  - summary (total, passed, failed, errors, pass_rate, target_met, regressions)
  - by_category (fetch/calculation/composite counts)
  - tasks array (full details including generated_code, stage_on_timeout)
  - security_results

**9. Modify run_task() to:**
- Track result["state"] as pass/fail/error (not just success boolean)
- Track result["generated_code"] from tool.code_content if available
- Track result["refiner_attempts"] if synthesis used refiner
- Track result["stage_on_timeout"] for timeout errors
- Print colored status: GREEN for pass, RED for fail, YELLOW for error

**10. Modify run_all_tasks() to:**
- Register SIGINT handler at start
- Set self.interrupted flag on Ctrl+C
- Break loop if interrupted
- Call save_results_json() at end (or on interrupt)

**11. Add print_summary() method:**
- Print colored summary with pass rate, regressions, security block rate
- Show per-category breakdown (fetch/calc/composite)
- Final verdict: "ALL CRITERIA MET" (green) or "CRITERIA NOT MET" (red)

**12. Modify generate_report() to:**
- Call save_results_json() after CSV generation
- Call print_summary() after report

**13. Add --clear-registry CLI flag:**
- If set, call clear_registry() before running tasks

**Important:** Keep existing CSV output for backwards compatibility. JSON is additional.
  </action>
  <verify>
```bash
cd fin_evo_agent

# Test 1: Verify Colors class exists
python3 -c "
from benchmarks.run_eval import Colors
print(f'{Colors.GREEN}PASS{Colors.RESET} - colors work')
print(f'{Colors.RED}FAIL{Colors.RESET} - colors work')
"

# Test 2: Verify ResultState class exists
python3 -c "
from benchmarks.run_eval import ResultState
assert ResultState.PASS == 'pass'
assert ResultState.FAIL == 'fail'
assert ResultState.ERROR == 'error'
print('PASS: ResultState class validated')
"

# Test 3: Verify baseline loading
python3 -c "
from benchmarks.run_eval import EvalRunner
runner = EvalRunner('evolving')
assert hasattr(runner, 'baseline'), 'baseline attribute missing'
assert len(runner.baseline.get('passing_tasks', [])) == 13
print('PASS: baseline loading validated')
"

# Test 4: Verify results directory can be created
python3 -c "
from pathlib import Path
results_dir = Path('benchmarks/results')
results_dir.mkdir(exist_ok=True)
assert results_dir.exists()
print('PASS: results directory exists')
"

# Test 5: Verify clear_registry method exists
python3 -c "
from benchmarks.run_eval import EvalRunner
runner = EvalRunner('evolving')
assert hasattr(runner, 'clear_registry'), 'clear_registry method missing'
print('PASS: clear_registry method exists')
"
```
  </verify>
  <done>run_eval.py has ResultState, Colors, baseline loading, JSON output, regression detection, interrupt handling, and clear_registry method</done>
</task>

</tasks>

<verification>
All verification commands pass:
1. baseline.json exists with 13 tasks and 0.80 target
2. Colors class produces colored terminal output
3. ResultState class has pass/fail/error constants
4. EvalRunner loads baseline on init
5. benchmarks/results/ directory can be created
6. clear_registry method exists on EvalRunner
</verification>

<success_criteria>
- baseline.json created with correct 13 task IDs
- run_eval.py enhanced with all features from CONTEXT.md decisions
- All verify commands pass without errors
- Backwards compatibility maintained (CSV output still works)
</success_criteria>

<output>
After completion, create `.planning/phases/04-regression-verification/04-01-SUMMARY.md`
</output>
