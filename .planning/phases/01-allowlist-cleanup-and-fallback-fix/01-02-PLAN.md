---
phase: 01-allowlist-cleanup-and-fallback-fix
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - fin_evo_agent/src/core/llm_adapter.py
autonomous: true

must_haves:
  truths:
    - "When the LLM API times out, the system returns an error result (not mock-generated RSI code)"
    - "Mock LLM only activates when API_KEY environment variable is unset, not on API errors or timeouts"
    - "Running without API_KEY still produces mock responses for testing"
    - "Error return dict includes text_response key for forward compatibility with refiner (REFNR-02)"
  artifacts:
    - path: "fin_evo_agent/src/core/llm_adapter.py"
      provides: "generate_tool_code method with correct error handling"
      contains: "def generate_tool_code"
  key_links:
    - from: "fin_evo_agent/src/core/llm_adapter.py:generate_tool_code"
      to: "error return dict"
      via: "except Exception handler returns error dict instead of calling _mock_generate"
      pattern: "LLM.*Error"
    - from: "fin_evo_agent/src/core/llm_adapter.py:generate_tool_code error return"
      to: "fin_evo_agent/src/evolution/synthesizer.py:synthesize"
      via: "synthesizer checks code_payload is None and handles it gracefully"
      pattern: "code_payload.*None"
    - from: "fin_evo_agent/src/core/llm_adapter.py:generate_tool_code error return"
      to: "fin_evo_agent/src/evolution/refiner.py:analyze_error"
      via: "refiner uses text_response key from result dict"
      pattern: "text_response"
---

<objective>
Fix the mock LLM fallback behavior so it only activates when no API key is configured, not on API errors or timeouts.

Purpose: The current code falls back to mock responses on ANY exception, including API timeouts. This masks real failures by returning hardcoded RSI code regardless of the actual task. After this fix, API errors will be reported honestly, while mock mode still works for local testing without an API key.

Output: Modified generate_tool_code() method that returns an error dict on API failures instead of falling back to mock.
</objective>

<execution_context>
@/home/node/.claude/get-shit-done/workflows/execute-plan.md
@/home/node/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-allowlist-cleanup-and-fallback-fix/01-RESEARCH.md

# Source file to modify
@fin_evo_agent/src/core/llm_adapter.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fix exception handler in generate_tool_code</name>
  <files>fin_evo_agent/src/core/llm_adapter.py</files>
  <action>
Modify the generate_tool_code() method (lines 116-140) to return an error dict on API exceptions instead of falling back to mock.

BEFORE (lines 116-140):
```python
if self.client is None:
    # Fallback to mock response if no API key
    raw_response = self._mock_generate(task)
else:
    try:
        completion = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt}
            ],
            temperature=self.temperature,
            extra_body={"enable_thinking": self.enable_thinking}
        )
        raw_response = completion.choices[0].message.content
    except Exception as e:
        print(f"[LLM Error] {e}, falling back to mock")
        raw_response = self._mock_generate(task)

parsed = self._clean_protocol(raw_response)
return {
    "thought_trace": parsed["thought_trace"],
    "code_payload": parsed["code_payload"],
    "raw_response": raw_response
}
```

AFTER:
```python
if self.client is None:
    # Mock only when no API key configured (testing mode)
    raw_response = self._mock_generate(task)
else:
    try:
        completion = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt}
            ],
            temperature=self.temperature,
            extra_body={"enable_thinking": self.enable_thinking}
        )
        raw_response = completion.choices[0].message.content
    except Exception as e:
        # API error or timeout - return error result, don't mask with mock
        print(f"[LLM Error] {e}")
        return {
            "thought_trace": "",
            "code_payload": None,
            "text_response": f"LLM API Error: {e}",
            "raw_response": f"LLM API Error: {e}"
        }

parsed = self._clean_protocol(raw_response)
return {
    "thought_trace": parsed["thought_trace"],
    "code_payload": parsed["code_payload"],
    "raw_response": raw_response
}
```

Key changes:
1. Remove `, falling back to mock` from the error print message
2. Replace `raw_response = self._mock_generate(task)` with early return of error dict
3. Error dict has: thought_trace="", code_payload=None, text_response="LLM API Error: {e}", raw_response="LLM API Error: {e}"
4. Include `text_response` key in error return for forward compatibility with refiner (REFNR-02), which accesses `result.get("text_response")` in analyze_error()
  </action>
  <verify>
```bash
cd /workspace/fin_evo_agent
# Verify the code structure is correct by checking the file
grep -A 6 "except Exception as e:" src/core/llm_adapter.py | grep -q "return" && echo "PASS: Exception handler returns instead of calling mock"
grep -A 6 "except Exception as e:" src/core/llm_adapter.py | grep -q "text_response" && echo "PASS: Error return includes text_response key"
```
  </verify>
  <done>The except Exception handler in generate_tool_code returns an error dict with thought_trace, code_payload, text_response, and raw_response keys instead of calling _mock_generate.</done>
</task>

<task type="auto">
  <name>Task 2: Verify mock still works without API key</name>
  <files></files>
  <action>
Test that mock mode still works when no API key is configured.

```bash
cd /workspace/fin_evo_agent
unset API_KEY
python3 -c "
from src.core.llm_adapter import LLMAdapter
adapter = LLMAdapter()
result = adapter.generate_tool_code('test task')
assert result['code_payload'] is not None, 'Mock should produce code'
print('PASS: Mock mode works without API key')
"
```

This verifies we didn't break the legitimate mock use case (testing without API key).
  </action>
  <verify>
```bash
cd /workspace/fin_evo_agent
unset API_KEY
python3 -c "from src.core.llm_adapter import LLMAdapter; a = LLMAdapter(); r = a.generate_tool_code('test'); assert r['code_payload'] is not None; print('PASS: Mock works')"
```
  </verify>
  <done>Running without API_KEY produces mock responses with valid code_payload.</done>
</task>

<task type="auto">
  <name>Task 3: Verify error return format matches expected structure and downstream compatibility</name>
  <files></files>
  <action>
Verify that the error return dict has the correct structure AND that downstream code handles it correctly.

The error return must have these keys:
- thought_trace: str (empty string)
- code_payload: None
- text_response: str (error message) -- required by refiner (REFNR-02) at refiner.py line 121
- raw_response: str (error message)

Additionally, verify downstream compatibility:
1. The synthesizer (synthesizer.py line 100) checks `if not result["code_payload"]` which handles `None` correctly
2. The refiner (refiner.py line 121) uses `result.get("text_response")` which requires the key to exist or gracefully defaults to None

```bash
cd /workspace/fin_evo_agent
python3 -c "
import ast
with open('src/core/llm_adapter.py', 'r') as f:
    content = f.read()

# Check that the error return has all required keys including text_response
assert 'thought_trace' in content, 'Missing thought_trace key'
assert 'code_payload' in content, 'Missing code_payload key'
assert 'text_response' in content, 'Missing text_response key'
assert 'raw_response' in content, 'Missing raw_response key'
assert 'LLM API Error' in content, 'Missing error message format'
print('PASS: Error return format has all 4 required keys')

# Verify synthesizer handles code_payload=None
with open('src/evolution/synthesizer.py', 'r') as f:
    synth_content = f.read()

# synthesizer.py line ~100: if not result['code_payload'] -- handles None correctly
assert 'not result[\"code_payload\"]' in synth_content or 'not result\[.code_payload.\]' in synth_content, \
    'Synthesizer does not check for code_payload being None/falsy'
print('PASS: Synthesizer handles code_payload=None')

# Verify refiner accesses text_response safely
with open('src/evolution/refiner.py', 'r') as f:
    refiner_content = f.read()

assert 'text_response' in refiner_content, 'Refiner references text_response'
print('PASS: Refiner uses text_response (available in error return)')
print('All downstream compatibility checks passed!')
"
```
  </action>
  <verify>
```bash
cd /workspace/fin_evo_agent
# Verify error return has all 4 keys
grep -A 6 "# API error or timeout" src/core/llm_adapter.py | grep -q "thought_trace" && \
grep -A 6 "# API error or timeout" src/core/llm_adapter.py | grep -q "code_payload" && \
grep -A 6 "# API error or timeout" src/core/llm_adapter.py | grep -q "text_response" && \
grep -A 6 "# API error or timeout" src/core/llm_adapter.py | grep -q "raw_response" && \
echo "PASS: Error return has all 4 required keys (thought_trace, code_payload, text_response, raw_response)"

# Verify synthesizer handles None code_payload
grep -q 'not result\["code_payload"\]' src/evolution/synthesizer.py && \
echo "PASS: Synthesizer checks for code_payload=None"

# Verify refiner uses text_response
grep -q 'text_response' src/evolution/refiner.py && \
echo "PASS: Refiner references text_response key"
```
  </verify>
  <done>The error return dict contains all 4 keys (thought_trace, code_payload, text_response, raw_response), the synthesizer handles code_payload=None without crashing, and the refiner can access text_response from the error return.</done>
</task>

</tasks>

<verification>
All verification commands must pass:

1. Exception handler returns error dict (not calling mock)
2. Error return includes `text_response` key for refiner compatibility
3. Mock mode works without API key (code_payload is not None)
4. Error return has correct structure (all four keys present: thought_trace, code_payload, text_response, raw_response)
5. Synthesizer handles code_payload=None without KeyError
6. Refiner can access text_response from error return dict

Additionally, the behavior can be tested manually:
- With API_KEY unset: `generate_tool_code()` returns mock response with valid code
- With API_KEY set but API failing: `generate_tool_code()` returns error dict with code_payload=None and text_response containing error message
</verification>

<success_criteria>
- MOCK-01 satisfied: On LLM timeout, return error result instead of falling back to mock
- MOCK-02 satisfied: Mock LLM only activates when no API key is configured
- Error return format includes all 4 keys (thought_trace, code_payload, text_response, raw_response) - no KeyError downstream
- text_response key present in error return for forward compatibility with refiner (REFNR-02)
- Mock testing mode still works for local development
- Synthesizer handles code_payload=None gracefully
</success_criteria>

<output>
After completion, create `.planning/phases/01-allowlist-cleanup-and-fallback-fix/01-02-SUMMARY.md`
</output>
