---
phase: 01-allowlist-cleanup-and-fallback-fix
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - fin_evo_agent/src/core/llm_adapter.py
autonomous: true

must_haves:
  truths:
    - "When the LLM API times out, the system returns an error result (not mock-generated RSI code)"
    - "Mock LLM only activates when API_KEY environment variable is unset, not on API errors or timeouts"
    - "Running without API_KEY still produces mock responses for testing"
  artifacts:
    - path: "fin_evo_agent/src/core/llm_adapter.py"
      provides: "generate_tool_code method with correct error handling"
      contains: "def generate_tool_code"
  key_links:
    - from: "fin_evo_agent/src/core/llm_adapter.py:generate_tool_code"
      to: "error return dict"
      via: "except Exception handler returns error dict instead of calling _mock_generate"
      pattern: "LLM.*Error"
---

<objective>
Fix the mock LLM fallback behavior so it only activates when no API key is configured, not on API errors or timeouts.

Purpose: The current code falls back to mock responses on ANY exception, including API timeouts. This masks real failures by returning hardcoded RSI code regardless of the actual task. After this fix, API errors will be reported honestly, while mock mode still works for local testing without an API key.

Output: Modified generate_tool_code() method that returns an error dict on API failures instead of falling back to mock.
</objective>

<execution_context>
@/home/node/.claude/get-shit-done/workflows/execute-plan.md
@/home/node/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-allowlist-cleanup-and-fallback-fix/01-RESEARCH.md

# Source file to modify
@fin_evo_agent/src/core/llm_adapter.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fix exception handler in generate_tool_code</name>
  <files>fin_evo_agent/src/core/llm_adapter.py</files>
  <action>
Modify the generate_tool_code() method (lines 116-140) to return an error dict on API exceptions instead of falling back to mock.

BEFORE (lines 116-140):
```python
if self.client is None:
    # Fallback to mock response if no API key
    raw_response = self._mock_generate(task)
else:
    try:
        completion = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt}
            ],
            temperature=self.temperature,
            extra_body={"enable_thinking": self.enable_thinking}
        )
        raw_response = completion.choices[0].message.content
    except Exception as e:
        print(f"[LLM Error] {e}, falling back to mock")
        raw_response = self._mock_generate(task)

parsed = self._clean_protocol(raw_response)
return {
    "thought_trace": parsed["thought_trace"],
    "code_payload": parsed["code_payload"],
    "raw_response": raw_response
}
```

AFTER:
```python
if self.client is None:
    # Mock only when no API key configured (testing mode)
    raw_response = self._mock_generate(task)
else:
    try:
        completion = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt}
            ],
            temperature=self.temperature,
            extra_body={"enable_thinking": self.enable_thinking}
        )
        raw_response = completion.choices[0].message.content
    except Exception as e:
        # API error or timeout - return error result, don't mask with mock
        print(f"[LLM Error] {e}")
        return {
            "thought_trace": "",
            "code_payload": None,
            "raw_response": f"LLM API Error: {e}"
        }

parsed = self._clean_protocol(raw_response)
return {
    "thought_trace": parsed["thought_trace"],
    "code_payload": parsed["code_payload"],
    "raw_response": raw_response
}
```

Key changes:
1. Remove `, falling back to mock` from the error print message
2. Replace `raw_response = self._mock_generate(task)` with early return of error dict
3. Error dict has: thought_trace="", code_payload=None, raw_response="LLM API Error: {e}"
  </action>
  <verify>
```bash
cd /workspace/fin_evo_agent
# Verify the code structure is correct by checking the file
grep -A 5 "except Exception as e:" src/core/llm_adapter.py | grep -q "return" && echo "PASS: Exception handler returns instead of calling mock"
```
  </verify>
  <done>The except Exception handler in generate_tool_code returns an error dict instead of calling _mock_generate.</done>
</task>

<task type="auto">
  <name>Task 2: Verify mock still works without API key</name>
  <files></files>
  <action>
Test that mock mode still works when no API key is configured.

```bash
cd /workspace/fin_evo_agent
unset API_KEY
python3 -c "
from src.core.llm_adapter import LLMAdapter
adapter = LLMAdapter()
result = adapter.generate_tool_code('test task')
assert result['code_payload'] is not None, 'Mock should produce code'
print('PASS: Mock mode works without API key')
"
```

This verifies we didn't break the legitimate mock use case (testing without API key).
  </action>
  <verify>
```bash
cd /workspace/fin_evo_agent
unset API_KEY
python3 -c "from src.core.llm_adapter import LLMAdapter; a = LLMAdapter(); r = a.generate_tool_code('test'); assert r['code_payload'] is not None; print('PASS: Mock works')"
```
  </verify>
  <done>Running without API_KEY produces mock responses with valid code_payload.</done>
</task>

<task type="auto">
  <name>Task 3: Verify error return format matches expected structure</name>
  <files></files>
  <action>
Verify that the error return dict has the correct structure by inspecting the code.

The error return must have these keys:
- thought_trace: str (empty string)
- code_payload: None
- raw_response: str (error message)

This matches the normal return structure so downstream code doesn't crash on KeyError.

```bash
cd /workspace/fin_evo_agent
python3 -c "
import ast
with open('src/core/llm_adapter.py', 'r') as f:
    content = f.read()

# Check that the error return has all required keys
assert 'thought_trace' in content, 'Missing thought_trace key'
assert 'code_payload' in content, 'Missing code_payload key'
assert 'raw_response' in content, 'Missing raw_response key'
assert 'LLM API Error' in content, 'Missing error message format'
print('PASS: Error return format is correct')
"
```
  </action>
  <verify>
```bash
cd /workspace/fin_evo_agent
grep -A 4 "# API error or timeout" src/core/llm_adapter.py | grep -q "thought_trace" && \
grep -A 4 "# API error or timeout" src/core/llm_adapter.py | grep -q "code_payload" && \
grep -A 4 "# API error or timeout" src/core/llm_adapter.py | grep -q "raw_response" && \
echo "PASS: Error return has all required keys"
```
  </verify>
  <done>The error return dict contains thought_trace, code_payload, and raw_response keys.</done>
</task>

</tasks>

<verification>
All verification commands must pass:

1. Exception handler returns error dict (not calling mock)
2. Mock mode works without API key (code_payload is not None)
3. Error return has correct structure (all three keys present)

Additionally, the behavior can be tested manually:
- With API_KEY unset: `generate_tool_code()` returns mock response with valid code
- With API_KEY set but API failing: `generate_tool_code()` returns error dict with code_payload=None
</verification>

<success_criteria>
- MOCK-01 satisfied: On LLM timeout, return error result instead of falling back to mock
- MOCK-02 satisfied: Mock LLM only activates when no API key is configured
- Error return format matches normal return format (no KeyError downstream)
- Mock testing mode still works for local development
</success_criteria>

<output>
After completion, create `.planning/phases/01-allowlist-cleanup-and-fallback-fix/01-02-SUMMARY.md`
</output>
