---
phase: 03-refiner-pipeline-repair
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - fin_evo_agent/src/core/llm_adapter.py
  - fin_evo_agent/src/evolution/refiner.py
autonomous: true

must_haves:
  truths:
    - "generate_tool_code() return dict includes 'text_response' key"
    - "Refiner's ERROR_PATTERNS includes ModuleNotFoundError, ImportError, and AssertionError"
    - "_classify_error() correctly identifies ModuleNotFoundError from stderr"
    - "analyze_error() prioritizes text_response over thought_trace for root cause"
  artifacts:
    - path: "fin_evo_agent/src/core/llm_adapter.py"
      provides: "text_response in generate_tool_code return dict"
      contains: '"text_response": parsed["text_response"]'
    - path: "fin_evo_agent/src/evolution/refiner.py"
      provides: "Complete ERROR_PATTERNS dict with all required error types"
      contains: "ModuleNotFoundError"
  key_links:
    - from: "LLMAdapter.generate_tool_code()"
      to: "Refiner.analyze_error()"
      via: "result['text_response']"
      pattern: "text_response extraction"
    - from: "Refiner._classify_error()"
      to: "ERROR_PATTERNS"
      via: "pattern matching"
      pattern: "error classification"
---

<objective>
Fix the refiner's error analysis pipeline by: (1) adding text_response to generate_tool_code() return dict, (2) adding missing error patterns (ModuleNotFoundError, ImportError, AssertionError), and (3) updating analyze_error() to prioritize text_response.

Purpose: The current refiner cannot properly analyze errors because it's missing text_response from LLM output and doesn't recognize common error types. This causes the repair loop to fail with "UnknownError" classification.

Output: Updated llm_adapter.py with text_response in return dict, and refiner.py with complete error patterns and improved analyze_error().
</objective>

<execution_context>
@/Users/liuzhenqian/.claude/get-shit-done/workflows/execute-plan.md
@/Users/liuzhenqian/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-refiner-pipeline-repair/03-RESEARCH.md
@.planning/phases/03-refiner-pipeline-repair/03-CONTEXT.md
@fin_evo_agent/src/core/llm_adapter.py
@fin_evo_agent/src/evolution/refiner.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add text_response to generate_tool_code() return dict</name>
  <files>fin_evo_agent/src/core/llm_adapter.py</files>
  <action>
In llm_adapter.py, modify the return statement in generate_tool_code() (around line 181-185) to include text_response:

Current return dict (lines 181-185):
```python
parsed = self._clean_protocol(raw_response)
return {
    "thought_trace": parsed["thought_trace"],
    "code_payload": parsed["code_payload"],
    "raw_response": raw_response
}
```

Change to:
```python
parsed = self._clean_protocol(raw_response)
return {
    "thought_trace": parsed["thought_trace"],
    "code_payload": parsed["code_payload"],
    "text_response": parsed["text_response"],
    "raw_response": raw_response
}
```

This is a one-line addition. The _clean_protocol() method already extracts text_response (line 119, 128), it just wasn't being passed through.
  </action>
  <verify>
```bash
cd "/Users/liuzhenqian/Desktop/personal project/2026-1-week3/Insitu finance agent/fin_evo_agent"

# Verify text_response is in the return dict structure at lines 181-185
# This checks the actual code path, not mock LLM runtime behavior
grep -A 5 'parsed = self._clean_protocol' src/core/llm_adapter.py | grep -q '"text_response"'
if [ $? -eq 0 ]; then
    echo "PASS: text_response found in parsed result return dict"
else
    echo "FAIL: text_response missing from return dict after _clean_protocol"
    exit 1
fi

# Double-check: the return dict should have all 4 keys including text_response
python3 -c "
import re
with open('src/core/llm_adapter.py', 'r') as f:
    content = f.read()

# Find the return dict after _clean_protocol call
pattern = r'parsed = self._clean_protocol.*?return \{[^}]+\}'
match = re.search(pattern, content, re.DOTALL)
if match:
    return_block = match.group()
    assert '\"text_response\"' in return_block, 'text_response not in return dict'
    print('PASS: text_response confirmed in generate_tool_code return dict')
else:
    raise AssertionError('Could not find return dict pattern')
"
```
Exit code 0 confirms success.
  </verify>
  <done>
generate_tool_code() now returns text_response in its result dict alongside thought_trace, code_payload, and raw_response.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add missing error patterns to ERROR_PATTERNS dict</name>
  <files>fin_evo_agent/src/evolution/refiner.py</files>
  <action>
In refiner.py, add three new error patterns to ERROR_PATTERNS dict (around lines 34-58).

Add these patterns after the existing entries:

```python
"ModuleNotFoundError": {
    "pattern": r"ModuleNotFoundError:.*No module named '(\w+)'",
    "strategy": "Replace forbidden module with pandas/numpy equivalent. FORBIDDEN: talib, yfinance, akshare, requests, os, sys, subprocess."
},
"ImportError": {
    "pattern": r"ImportError:.*",
    "strategy": "Check import path and module name. Use only: pandas, numpy, datetime, json, math, decimal, collections, re, typing."
},
"AssertionError": {
    "pattern": r"AssertionError:?(.*)?",
    "strategy": "Fix the calculation logic to match expected output. Do NOT modify the test assertions."
},
```

Insert these before the closing brace of ERROR_PATTERNS, maintaining the existing patterns (TypeError, KeyError, IndexError, ValueError, ZeroDivisionError, AttributeError).
  </action>
  <verify>
```bash
cd "/Users/liuzhenqian/Desktop/personal project/2026-1-week3/Insitu finance agent/fin_evo_agent"

# Verify all required error patterns exist
python3 -c "
from src.evolution.refiner import Refiner
r = Refiner()
required = ['ModuleNotFoundError', 'ImportError', 'AssertionError', 'TypeError', 'KeyError', 'ValueError']
for err_type in required:
    assert err_type in r.ERROR_PATTERNS, f'{err_type} missing from ERROR_PATTERNS'
print(f'PASS: All {len(required)} required error patterns present')
"
```
Exit code 0 confirms success.
  </verify>
  <done>
ERROR_PATTERNS dict now includes ModuleNotFoundError, ImportError, and AssertionError alongside existing patterns.
  </done>
</task>

<task type="auto">
  <name>Task 3: Verify _classify_error() correctly identifies new error types</name>
  <files>None (verification only)</files>
  <action>
Test that the new error patterns are correctly matched by _classify_error().

This is a verification-only task to ensure the patterns work as expected. No code changes needed if Task 2 was done correctly.
  </action>
  <verify>
```bash
cd "/Users/liuzhenqian/Desktop/personal project/2026-1-week3/Insitu finance agent/fin_evo_agent"

# Test error classification for all new types
python3 -c "
from src.evolution.refiner import Refiner
r = Refiner()

# Test ModuleNotFoundError
err_type, strategy = r._classify_error(\"ModuleNotFoundError: No module named 'talib'\")
assert err_type == 'ModuleNotFoundError', f'Expected ModuleNotFoundError, got {err_type}'
print(f'PASS: ModuleNotFoundError classified correctly')

# Test ImportError
err_type, strategy = r._classify_error(\"ImportError: cannot import name 'RSI' from 'talib'\")
assert err_type == 'ImportError', f'Expected ImportError, got {err_type}'
print(f'PASS: ImportError classified correctly')

# Test AssertionError
err_type, strategy = r._classify_error('AssertionError: expected 75.0, got 50.0')
assert err_type == 'AssertionError', f'Expected AssertionError, got {err_type}'
print(f'PASS: AssertionError classified correctly')

print('All error classifications working correctly')
"
```
Exit code 0 confirms success.
  </verify>
  <done>
_classify_error() correctly identifies ModuleNotFoundError, ImportError, and AssertionError from stderr messages.
  </done>
</task>

<task type="auto">
  <name>Task 4: Update analyze_error() to prioritize text_response with truncation</name>
  <files>fin_evo_agent/src/evolution/refiner.py</files>
  <action>
In refiner.py, modify the analyze_error() method (around line 121) to prioritize text_response over thought_trace and add truncation.

Current code (line 121):
```python
root_cause = result.get("thought_trace") or result.get("text_response") or f"{error_type}: {strategy}"
```

Replace with:
```python
# Extract text_response (LLM explanation) and thought_trace (internal reasoning)
text_response = result.get("text_response", "")
thought_trace = result.get("thought_trace", "")

# Truncate to reasonable length to keep prompts manageable
MAX_TEXT_LEN = 2000
if len(text_response) > MAX_TEXT_LEN:
    text_response = text_response[:MAX_TEXT_LEN//2] + "\n...[truncated]...\n" + text_response[-MAX_TEXT_LEN//2:]

# Priority: text_response (explanation) > thought_trace (reasoning) > default
root_cause = text_response or thought_trace or f"{error_type}: {strategy}"
```

This ensures text_response is prioritized and truncated appropriately.
  </action>
  <verify>
```bash
cd "/Users/liuzhenqian/Desktop/personal project/2026-1-week3/Insitu finance agent/fin_evo_agent"

# CRITICAL: Verify text_response is prioritized FIRST (before thought_trace)
# The exact pattern must be: root_cause = text_response or thought_trace
# NOT: root_cause = thought_trace or text_response (wrong order!)

# Check for correct priority order using grep
if grep -q 'root_cause = text_response or thought_trace' src/evolution/refiner.py; then
    echo "PASS: text_response correctly prioritized BEFORE thought_trace"
else
    echo "FAIL: text_response must appear BEFORE thought_trace in root_cause assignment"
    echo "Expected pattern: 'root_cause = text_response or thought_trace'"
    echo "Current content:"
    grep -n 'root_cause' src/evolution/refiner.py
    exit 1
fi

# Verify truncation logic is present
if grep -q 'MAX_TEXT_LEN' src/evolution/refiner.py; then
    echo "PASS: MAX_TEXT_LEN truncation logic present"
else
    echo "FAIL: MAX_TEXT_LEN truncation logic missing"
    exit 1
fi

# Verify text_response variable is extracted before use
if grep -q 'text_response = result.get' src/evolution/refiner.py; then
    echo "PASS: text_response extraction present"
else
    echo "FAIL: text_response extraction missing"
    exit 1
fi
```
Exit code 0 confirms success.
  </verify>
  <done>
analyze_error() now prioritizes text_response over thought_trace, with truncation at 2000 characters.
  </done>
</task>

</tasks>

<verification>
After all tasks complete, run this comprehensive verification:

```bash
cd "/Users/liuzhenqian/Desktop/personal project/2026-1-week3/Insitu finance agent/fin_evo_agent"

# 1. Verify text_response in generate_tool_code return
python3 -c "
from src.core.llm_adapter import LLMAdapter
a = LLMAdapter()
r = a.generate_tool_code('test')
assert 'text_response' in r, 'FAIL: text_response missing'
print('[1/4] PASS: text_response in return dict')
"

# 2. Verify all error patterns exist
python3 -c "
from src.evolution.refiner import Refiner
r = Refiner()
required = ['ModuleNotFoundError', 'ImportError', 'AssertionError']
for e in required:
    assert e in r.ERROR_PATTERNS, f'FAIL: {e} missing'
print('[2/4] PASS: All new error patterns present')
"

# 3. Verify error classification works
python3 -c "
from src.evolution.refiner import Refiner
r = Refiner()
err_type, _ = r._classify_error(\"ModuleNotFoundError: No module named 'talib'\")
assert err_type == 'ModuleNotFoundError', f'FAIL: got {err_type}'
print('[3/4] PASS: ModuleNotFoundError classification works')
"

# 4. Verify analyze_error has text_response handling
python3 -c "
import inspect
from src.evolution.refiner import Refiner
r = Refiner()
source = inspect.getsource(r.analyze_error)
assert 'text_response' in source, 'FAIL: text_response not in analyze_error'
print('[4/4] PASS: analyze_error has text_response handling')
"

echo ""
echo "=== All Plan 01 verifications passed ==="
```
</verification>

<success_criteria>
Phase 3 Plan 01 is complete when:

1. [x] text_response is included in generate_tool_code() return dict
2. [x] ERROR_PATTERNS includes ModuleNotFoundError, ImportError, AssertionError
3. [x] _classify_error() correctly identifies new error types
4. [x] analyze_error() prioritizes text_response over thought_trace with truncation

All verification commands exit with code 0.
</success_criteria>

<output>
After completion, create `.planning/phases/03-refiner-pipeline-repair/03-01-SUMMARY.md`
</output>
